{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10948240511523478498\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 8602255617191097737\n",
      "physical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 14:11:55.998021: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
      "2024-09-24 14:11:55.998038: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
      "2024-09-24 14:11:55.998042: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
      "2024-09-24 14:11:55.998073: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-09-24 14:11:55.998085: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check for MPS support (Metal GPU backend)\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU not found. Trying MPS...\")\n",
    "    try:\n",
    "        mps_devices = tf.config.experimental.list_physical_devices('MPS')\n",
    "        if mps_devices:\n",
    "            print(\"MPS device found:\", mps_devices)\n",
    "        else:\n",
    "            print(\"MPS device not found\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return gray image\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thumb Up\n",
      "Thumb Up\n"
     ]
    }
   ],
   "source": [
    "# training targets\n",
    "targets_df = pd.read_csv('data_csv/train.csv', header=None, sep=\",\", index_col=0)\n",
    "targets = targets_df[1].to_dict()\n",
    "print(targets[1157])\n",
    "\n",
    "# validation targets\n",
    "targets_validation_df = pd.read_csv('data_csv/validation.csv', header=None, sep=\",\", index_col=0)\n",
    "targets_validation = targets_validation_df[1].to_dict()\n",
    "print(targets_validation[8043])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thumb Up'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_validation[8043]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes label you want to use all labels \n",
    "'''label = pd.read_csv('data_csv/labels.csv',header=None, usecols=[0])\n",
    "label.head()\n",
    "targets_name = label[0].tolist()\n",
    "len(targets_name)'''\n",
    "\n",
    "# The classes (labels) we want to use\n",
    "targets_name = [\n",
    "    \"Swiping Right\",\n",
    "    \"Sliding Two Fingers Left\",\n",
    "    \"No gesture\",\n",
    "    \"Thumb Up\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "971"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data directories\n",
    "path = \"training_samples/\"\n",
    "path_cv = \"validation_samples/\"\n",
    "\n",
    "dirs = os.listdir(path)\n",
    "dirs_cv = os.listdir(path_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "971\n"
     ]
    }
   ],
   "source": [
    "# number of samples for training and validation\n",
    "print(len(dirs))\n",
    "print(len(dirs_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_count = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The videos do not have the same number of frames, here we try to unify.\n",
    "'''\n",
    "hm_frames = 15 # number of frames\n",
    "# unify number of frames for each training\n",
    "def get_unify_frames(path):\n",
    "    offset = 0\n",
    "    # pick frames\n",
    "    frames = os.listdir(path)\n",
    "    frames_count = len(frames)\n",
    "    # unify number of frames \n",
    "    if hm_frames > frames_count:\n",
    "        # duplicate last frame if video is shorter than necessary\n",
    "        frames += [frames[-1]] * (hm_frames - frames_count)\n",
    "    elif hm_frames < frames_count:\n",
    "        # If there are more frames, then sample starting offset\n",
    "        #diff = (frames_count - hm_frames)\n",
    "        #offset = diff-1 \n",
    "        frames = frames[0:hm_frames]\n",
    "    return frames  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize frames\n",
    "def resize_frame(frame):\n",
    "    frame = img.imread(frame)\n",
    "    frame = cv2.resize(frame, (64, 64))\n",
    "    return frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust training data\n",
    "counter_training = 0 # number of training\n",
    "training_targets = [] # training targets \n",
    "new_frames = [] # training data after resize & unify\n",
    "for directory in dirs:\n",
    "    new_frame = [] # one training\n",
    "    # Frames in each folder\n",
    "    frames = get_unify_frames(path+directory)\n",
    "    if len(frames) == hm_frames: # just to be sure\n",
    "        for frame in frames:\n",
    "            frame = resize_frame(path+directory+'/'+frame)\n",
    "            new_frame.append(rgb2gray(frame))\n",
    "            if len(new_frame) == 15: # partition each training on two trainings.\n",
    "                new_frames.append(new_frame) # append each partition to training data\n",
    "                training_targets.append(targets_name.index(targets[int(directory)]))\n",
    "                counter_training +=1\n",
    "                new_frame = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do the same for the validation data\n",
    "counter_validation = 0\n",
    "cv_targets = []\n",
    "new_frames_cv = []\n",
    "for directory in dirs_cv:\n",
    "    new_frame = []\n",
    "    # Frames in each folder\n",
    "    frames = get_unify_frames(path_cv+directory)\n",
    "    if len(frames)==hm_frames:\n",
    "        for frame in frames:\n",
    "            frame = resize_frame(path_cv+directory+'/'+frame)\n",
    "            new_frame.append(rgb2gray(frame))\n",
    "            if len(new_frame) == 15:\n",
    "                new_frames_cv.append(new_frame)\n",
    "                cv_targets.append(targets_name.index(targets_validation[int(directory)]))\n",
    "                counter_validation +=1\n",
    "                new_frame = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# To check training length\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mnew_frames\u001b[49m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(training_targets))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_frames' is not defined"
     ]
    }
   ],
   "source": [
    "# To check training length\n",
    "print(len(new_frames))\n",
    "print(len(training_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1942\n",
      "1942\n"
     ]
    }
   ],
   "source": [
    "# To check validation length\n",
    "print(len(new_frames_cv))\n",
    "print(len(cv_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 2, 2, 0, 0, 3, 3, 0, 0, 1, 1, 3, 3, 3, 3, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_targets[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Resizes, converts to grayscale, and normalizes the frame.\"\"\"\n",
    "    frame = resize_frame(frame)  # Resize to 64x64\n",
    "    frame = rgb2gray(frame)  # Convert to grayscale\n",
    "    return frame\n",
    "\n",
    "def frame_generator(dirs, path, targets, targets_name, batch_size=32):\n",
    "    \"\"\"Generator to yield batches of frames and corresponding targets.\"\"\"\n",
    "    scaler = StandardScaler()  # Initialize scaler\n",
    "\n",
    "    while True:\n",
    "        batch_frames = []  # Store batch of frames\n",
    "        batch_targets = []  # Store batch of targets\n",
    "\n",
    "        for directory in dirs:\n",
    "            frames = get_unify_frames(os.path.join(path, directory))  # Unify frame count to 15 frames\n",
    "            \n",
    "            processed_frames = []\n",
    "            for frame in frames:\n",
    "                processed_frame = preprocess_frame(os.path.join(path, directory, frame))  # Preprocess each frame\n",
    "                processed_frames.append(processed_frame)\n",
    "            \n",
    "            # Stack frames and reshape to (15, 64, 64, 1) for Conv3D\n",
    "            processed_frames = np.stack(processed_frames).reshape(15, 64, 64, 1)\n",
    "\n",
    "            # Normalize using StandardScaler across the frames\n",
    "            processed_frames = scaler.fit_transform(processed_frames.reshape(-1, 64 * 64))  # Normalize pixel values\n",
    "            processed_frames = processed_frames.reshape(15, 64, 64, 1)\n",
    "\n",
    "            batch_frames.append(processed_frames)\n",
    "            batch_targets.append(targets_name.index(targets[int(directory)]))\n",
    "\n",
    "            # Once we have enough frames for a batch, yield them\n",
    "            if len(batch_frames) == batch_size:\n",
    "                yield np.array(batch_frames), np.array(batch_targets)\n",
    "                batch_frames, batch_targets = [], []  # Reset for next batch\n",
    "\n",
    "def create_dataset(dirs, path, targets, targets_name, batch_size=32):\n",
    "    \"\"\"Creates a tf.data.Dataset from the generator.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: frame_generator(dirs, path, targets, targets_name, batch_size),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(batch_size, 15, 64, 64, 1), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(batch_size,), dtype=tf.int32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Shuffle, batch, and prefetch\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#show data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m num,frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_frames[i][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m18\u001b[39m]):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#show data\n",
    "fig = plt.figure()\n",
    "for i in range(2,4):\n",
    "    for num,frame in enumerate(new_frames[i][0:18]):\n",
    "        y = fig.add_subplot(4,5,num+1)\n",
    "        y.imshow(frame, cmap='gray')\n",
    "    fig = plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# convert training data to np float32\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m training_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mnew_frames\u001b[49m[\u001b[38;5;241m0\u001b[39m:counter_training], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_frames' is not defined"
     ]
    }
   ],
   "source": [
    "# convert training data to np float32\n",
    "training_data = np.array(new_frames[0:counter_training], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to empty the RAM\n",
    "def release_list(a):\n",
    "   del a[:]\n",
    "   del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_list(new_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert validation data to np float32\n",
    "cv_data = np.array(new_frames_cv[0:counter_validation], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_list(new_frames_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 15, 64, 64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1942, 15, 64, 64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old mean 115.14039\n",
      "new mean -4.5216747e-10\n",
      "(12000, 15, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# Normalisation: training\n",
    "print('old mean', training_data.mean())\n",
    "scaler = StandardScaler()\n",
    "scaled_images  = scaler.fit_transform(training_data.reshape(-1, 15*64*64))\n",
    "print('new mean', scaled_images.mean())\n",
    "scaled_images  = scaled_images.reshape(-1, 15, 64, 64, 1)\n",
    "print(scaled_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old mean 111.76559\n",
      "new mean -1.5796356e-08\n",
      "(1942, 15, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# Normalisation: validation\n",
    "print('old mean', cv_data.mean())\n",
    "scaler = StandardScaler()\n",
    "scaled_images_cv  = scaler.fit_transform(cv_data.reshape(-1, 15*64*64))\n",
    "print('new mean',scaled_images_cv.mean())\n",
    "scaled_images_cv  = scaled_images_cv.reshape(-1, 15, 64, 64, 1)\n",
    "print(scaled_images_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3DModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Conv3DModel, self).__init__()\n",
    "        # Convolutions\n",
    "        self.conv1 = tf.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "        self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\n",
    "        self.conv2 = tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=\"conv2\", data_format='channels_last')\n",
    "        self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\n",
    "        \n",
    "        # LSTM & Flatten\n",
    "        self.convLSTM = tf.keras.layers.ConvLSTM2D(40, (3, 3))\n",
    "        self.flatten = tf.keras.layers.Flatten(name=\"flatten\")\n",
    "\n",
    "        # Dense layers\n",
    "        self.d1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\n",
    "        self.out = tf.keras.layers.Dense(4, activation='softmax', name=\"output\")\n",
    "\n",
    "    def call(self, x):\n",
    "        # Reshape input: ensure (batch_size, 15, 64, 64, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.convLSTM(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Input tensor shape should be (batch_size, 15, 64, 64, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model\n",
    "class Conv3DModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Conv3DModel, self).__init__()\n",
    "    # Convolutions\n",
    "    self.conv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\n",
    "    self.conv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')\n",
    "   \n",
    "    # LSTM & Flatten\n",
    "    self.convLSTM = tf.keras.layers.ConvLSTM2D(40, (3, 3), padding='same', return_sequences=False)\n",
    "    # self.convLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))\n",
    "    self.flatten =  tf.keras.layers.Flatten(name=\"flatten\")\n",
    "\n",
    "    # Dense layers\n",
    "    self.d1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\n",
    "    #self.out = tf.keras.layers.Dense(4, activation='softmax', name=\"output\")\n",
    "    self.out = tf.keras.layers.Dense(4, activation='softmax')\n",
    "\n",
    "    \n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.pool1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.pool2(x)\n",
    "    x = self.convLSTM(x)\n",
    "    #x = self.pool2(x)\n",
    "    #x = self.conv3(x)\n",
    "    #x = self.pool3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# Create the training dataset\n",
    "train_dataset = create_dataset(dirs, path, targets, targets_name, batch_size)\n",
    "\n",
    "# Create the validation dataset\n",
    "val_dataset = create_dataset(dirs_cv, path, targets_validation, targets_name, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 14:56:59.683226: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:150: Filling up shuffle buffer (this may take a while): 2 of 1000\n",
      "2024-09-24 14:57:12.144369: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:150: Filling up shuffle buffer (this may take a while): 4 of 1000\n",
      "2024-09-24 14:57:29.697969: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:150: Filling up shuffle buffer (this may take a while): 7 of 1000\n",
      "2024-09-24 14:57:40.983078: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:150: Filling up shuffle buffer (this may take a while): 9 of 1000\n",
      "2024-09-24 14:57:58.160604: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:150: Filling up shuffle buffer (this may take a while): 12 of 1000\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "batch_size = 1000\n",
    "# Create the training dataset\n",
    "train_dataset = create_dataset(dirs, path, targets, targets_name, batch_size)\n",
    "\n",
    "# Create the validation dataset\n",
    "val_dataset = create_dataset(dirs_cv, path_cv, targets_validation, targets_name, batch_size)\n",
    "\n",
    "# Compile the model\n",
    "model = Conv3DModel()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the dataset\n",
    "with tf.device('/device:GPU:0'):\n",
    "    history = model.fit(\n",
    "        train_dataset, \n",
    "        epochs=10, \n",
    "        steps_per_epoch=len(dirs) // batch_size,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=len(dirs_cv) // batch_size\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv3DModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the loss and optimizer methods\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(scaled_images)\n",
    "y_train = np.array(training_targets)\n",
    "x_val = np.array(scaled_images_cv)\n",
    "y_val = np.array(cv_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 15, 64, 64, 1)\n",
      "(12000,)\n",
      "(1942, 15, 64, 64, 1)\n",
      "(1942,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here I use the checkpoints\n",
    "read more:\n",
    "https://www.tensorflow.org/beta/guide/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the epoch in the file name. (uses `str.format`)\n",
    "checkpoint_path = \"training_today/cp-{epoch:04d}.weights.h5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m272/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2:13\u001b[0m 1s/step - accuracy: 0.5297 - loss: 1.0047"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the training \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcp_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the training \n",
    "history = model.fit(x_train, y_train,\n",
    "                    callbacks = [cp_callback],\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    batch_size=32,\n",
    "                    epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# just after one epoch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# just after one epoch\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model for use in the application\n",
    "model.save_weights('weights/path_to_my_weights', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. use a more complex syntax, for experts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My model\n",
    "class Conv3DModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Conv3DModel, self).__init__()\n",
    "    # Convolutions\n",
    "    self.conv1 = tf.compat.v2.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), data_format='channels_last')\n",
    "    self.conv2 = tf.compat.v2.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', name=\"conv1\", data_format='channels_last')\n",
    "    self.pool2 = tf.keras.layers.MaxPool3D(pool_size=(2, 2,2), data_format='channels_last')\n",
    "   \n",
    "    # LSTM & Flatten\n",
    "    self.convLSTM =tf.keras.layers.ConvLSTM2D(40, (3, 3))\n",
    "    self.flatten =  tf.keras.layers.Flatten(name=\"flatten\")\n",
    "\n",
    "    # Dense layers\n",
    "    self.d1 = tf.keras.layers.Dense(128, activation='relu', name=\"d1\")\n",
    "    self.out = tf.keras.layers.Dense(4, activation='softmax', name=\"output\")\n",
    "    \n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.pool1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.pool2(x)\n",
    "    x = self.convLSTM(x)\n",
    "    #x = self.pool2(x)\n",
    "    #x = self.conv3(x)\n",
    "    #x = self.pool3(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv3DModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tensorflow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((scaled_images, training_targets))\n",
    "cv_dataset = tf.data.Dataset.from_tensor_slices((scaled_images_cv, cv_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=443, shape=(2, 4), dtype=float32, numpy=\n",
       "array([[0.22843026, 0.25249583, 0.25405762, 0.26501626],\n",
       "       [0.23236132, 0.2505814 , 0.2503527 , 0.26670453]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(scaled_images[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv3d_model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               multiple                  896       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv3D)               multiple                  55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv_lst_m2d_2 (ConvLSTM2D)  multiple                  149920    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "d1 (Dense)                   multiple                  737408    \n",
      "_________________________________________________________________\n",
      "output (Dense)               multiple                  516       \n",
      "=================================================================\n",
      "Total params: 944,100\n",
      "Trainable params: 944,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "# Accuracy\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(image, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Make a prediction on all the batch\n",
    "        predictions = model(image)\n",
    "        # Get the error/loss on these predictions\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    # Compute the gradient which respect to the loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    # Change the weights of the model\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # The metrics are accumulate over time. You don't need to average it yourself.\n",
    "    train_loss(loss)\n",
    "    train_accuracy(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(image, targets):\n",
    "    predictions = model(image)\n",
    "    t_loss = loss_fn(targets, predictions)\n",
    "    # Set the metrics for the test\n",
    "    valid_loss(t_loss)\n",
    "    valid_accuracy(targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### here I use the checkpoints\n",
    "read more:\n",
    "https://www.tensorflow.org/beta/guide/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x278b6d03630>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, 'training_checkpoints/tf_ckpts', max_to_keep=10)\n",
    "ckpt.restore(manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "batch_size = 32\n",
    "b = 0\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "for epoch in range(epoch):\n",
    "    # Training set\n",
    "    for images_batch, targets_batch in train_dataset.batch(batch_size):\n",
    "        train_step(images_batch, targets_batch)\n",
    "        template = '\\r Batch {}/{}, Loss: {}, Accuracy: {}'\n",
    "        print(template.format(\n",
    "            b, len(training_targets), train_loss.result(), \n",
    "            train_accuracy.result()*100\n",
    "        ), end=\"\")\n",
    "        b += batch_size\n",
    "    # Validation set\n",
    "    for images_batch, targets_batch in cv_dataset.batch(batch_size):\n",
    "        valid_step(images_batch, targets_batch)\n",
    "\n",
    "    template = '\\nEpoch {}, Valid Loss: {}, Valid Accuracy: {}'\n",
    "    print(template.format(\n",
    "        epoch+1,\n",
    "        valid_loss.result(), \n",
    "        valid_accuracy.result()*100)\n",
    "    )\n",
    "    training_acc.append(float(train_accuracy.result()*100))\n",
    "    validation_acc.append(float(valid_accuracy.result()*100))\n",
    "    ckpt.step.assign_add(1)\n",
    "    save_path = manager.save()\n",
    "    print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "    valid_loss.reset_states()\n",
    "    valid_accuracy.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    train_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training_checkpoints/tf_ckpts\\\\ckpt-1', 'training_checkpoints/tf_ckpts\\\\ckpt-2', 'training_checkpoints/tf_ckpts\\\\ckpt-3', 'training_checkpoints/tf_ckpts\\\\ckpt-4', 'training_checkpoints/tf_ckpts\\\\ckpt-5', 'training_checkpoints/tf_ckpts\\\\ckpt-6', 'training_checkpoints/tf_ckpts\\\\ckpt-7', 'training_checkpoints/tf_ckpts\\\\ckpt-8', 'training_checkpoints/tf_ckpts\\\\ckpt-9']\n"
     ]
    }
   ],
   "source": [
    "print(manager.checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plote Accuracy / epoch\n",
    "plt.plot([1,2,3,4,5,6,7,8,9],training_acc, '-' )\n",
    "plt.plot([1,2,3,4,5,6,7,8,9],validation_acc, '-' )\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model for use in the application\n",
    "model.save_weights('weights/path_to_my_weights', save_format='tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
